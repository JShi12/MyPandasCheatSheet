{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6e126b-56b1-4df2-8d0d-76c1ddc55a72",
   "metadata": {},
   "source": [
    "# Pandas Cheat Sheet for Data Science\n",
    "\n",
    " 1. This is a cheat sheet I prepared for data science coding tests on Pandas. It contains most common functions.\n",
    " 2. It is based on the cheast cheet from DATAQUEST (https://www.dataquest.io/blog/pandas-cheat-sheet/), I deleted some that are not commonly used or have been eprecated, and added some that are not included there.\n",
    " 3. The last part \"Datetime in Pandas part\" is included as time date is commonly seen in varioius analytical projects; but they might be less likely tested in coding tests.\n",
    " 4. To keep it impact, no dataframe sample is given to test the code. Another notebook with dataframe samples and results output are provided separately as **PandasBasicsWithExamples.ipynb**\n",
    " 5. This cheat sheet is also available in pdf format as **PandasCheatSheet.pdf**\n",
    "### Note: \n",
    "df - A pandas DataFrame object\n",
    "\n",
    "s - A pandas Series object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa3f92c-9cf1-4194-8fc0-85317dd051a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import packages for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# For displaying all of the columns in dataframes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbce389-29a9-42d9-b792-7d0a9e0a370e",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c7072-963f-46af-800b-f64c617f2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(filename) # From a CSV file\n",
    "pd.read_excel(file_path) # From a .xlsx\n",
    "pd.DataFrame(dict)    # From a dict, keys for column names, and values for data as lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edbe56-bdf5-40ca-a6cc-f763aedc2c6f",
   "metadata": {},
   "source": [
    "# Viewing/inspecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571e4e5-2425-49ce-886e-c0e577309b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n)\n",
    "df.tail(n)\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe() # Summary statistics for numerical columns\n",
    "df.isna().sum() # Check whether there are null values\n",
    "df.notna()\n",
    "s.nunique() # Return number of unique elements in the object\n",
    "s.unique()  # Return unique values of Series object\n",
    "s.value_counts(dropna=False) #View unique values and counts; Return a Series \n",
    "df.value_counts(dropna=False, normalize=True,ascending=True) #Return a Series containing the frequency of each distinct row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381f796-7c6b-47f4-8965-2fb8421b6f0d",
   "metadata": {},
   "source": [
    "# Selection and reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59323ba6-0d29-4cb5-8e80-53b3e5b3cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing with [], work on rows for dataframe\n",
    "s[:5] \n",
    "df[:5] # Return first 5 rows\n",
    "df[::2]# Return dataframe with a step of 2 on row selection\n",
    "df[::-1] # Return a dataframe with reversed order\n",
    "\n",
    "# Select columns and rows\n",
    "df[\"col\"] # Return the column \"col\" as a `Series`\n",
    "df[[\"col\"]] # Return the column \"col\" as a `DataFrame`\n",
    "df[[\"col1\",\"col2\"]]\n",
    "df[df['col1']>0] # Return the rows where the values of 'col1'>0\n",
    "df[df['col1']>0]['col2'] # Return 'col2' on selected rows based on values of 'col1' as a Series\n",
    "df[df['col1']>0][['col2']] # Return 'col2' on selected rows based on values of 'col1' as a DataFrame\n",
    "\n",
    "\n",
    "# Select by label via .loc[]\n",
    "df.loc[0]  #Return a Series -- the row with index 0\n",
    "df.loc[[0]] #Return a DataFrame -- the row with index 0\n",
    "df.loc[[0,3]] #Return a DataFrame -- the rows with integer index 0 and 1\n",
    "df.loc[:5] # Return a DataFrame with rows index from 0 to 5 (note include index 5)\n",
    "df.loc[\"row1\":\"row2\"] # Return a DataFrame with rows index (label) from \"row1\" to \"row2\"\n",
    "df.loc[:5,\"col1\":\"col2\"] # Return a DataFrame with rows index from 0 to 5, columns index between \"col1\" and \"col2\"\n",
    "df.loc[[\"a\", \"c\"],\"col2\":] # Return a DataFrame with rows index \"a\" and \"c\", columns index from \"col2\" to the end\n",
    "df.loc['a', 'A'] # Return a cell value at row \"a\", col \"A\", \n",
    "df.loc[:,\"col1\"]>0 # Return a boolean type Series along rows \n",
    "df.loc[df.loc[:,\"col1\"]>0,:] # Return a DataFrame with rows selected by a boolen array \n",
    "df.loc[df.col1>0,:] # Return a DataFrame with rows selected by a boolen array, a simplified way\n",
    "df.loc[lambda df: df.col1>0,:] # Selection by Callable lambda function\n",
    "#lambda with .loc be more useful in complex operations or when chaining multiple methods together, as it allows you to pass functions dynamically.\n",
    "df.loc[lambda df: (df.col1>0) & (df.col2==\"2009-01-02\"),:].assign(col3=lambda df: df.col1 - 2, col4=lambda df: df.col3*2) \n",
    "\n",
    "\n",
    "#Select by position via .iloc[], integer based indexing\n",
    "df.iloc[0]  #Return a Series -- the first row\n",
    "df.iloc[[0]] #Return a DataFrame -- the first row\n",
    "df.iloc[[0,1]] # Return a table with the first and second rows\n",
    "df.iloc[:4, :4] # Return the top_left_corner (first four rows, and first four columns)\n",
    "df.iloc[-4:, -4:] #Return the bottom_right_corner\n",
    "df.iloc[[1, 3, 5], [1, 3]]\n",
    "df.iloc[0,0] # Retrun the cell value at first row,first column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091942c-24bc-4b2c-8b78-7537b4f5d329",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5321ae0d-6b34-4d13-b0a6-32593dd4a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column or index names\n",
    "df.columns= ['new1', 'new2', 'new3'] # Rename columns\n",
    "df.rename(columns={\"old1\":\"new1\",\"old2\":\"new2\"}) # Selective renaming using a mapping \n",
    "df.rename(index={0: \"x\", 1: \"y\",\n",
    "                 \n",
    "                 2: \"z\"}) #Rename index using a mapping\n",
    "df.rename(index=lambda x: x+1) # Mass renaming index\n",
    "df.set_index(\"col1\") # Set the DataFrame index using existing column \"col\"\n",
    "\n",
    "# Dealing with null values\n",
    "df.isna() #Return a boolean same-sized object \n",
    "df[\"col1\"].isna() # Return a Series containing booleans\n",
    "df.notna() # opposite of df.isna()\n",
    "df.dropna() # Drop all rows that contain null values, default inplace=False\n",
    "df.dropna(inplace=True)\n",
    "df.dropna().reset_index() # Drops rows with null values and reset index\n",
    "df.dropna(axis=1) # Drop columns that contain null values\n",
    "df.dropna(how='all') # Drop rows when all elements are missing ; Default= \"any\" (at least one element is missing)\n",
    "df.dropna(thresh=n) # Drop rows that have less than thresh n non-null values\n",
    "df.fillna(x) # Replace all null values with x\n",
    "df.fillna(value={\"A\": 0, \"B\": 1}) # Replace all NaN elements in column ‘A’, ‘B’,with 0, 1 respectively.\n",
    "s.fillna(s.mean())\n",
    "df.fillna(df.mean(axis=0)) # Fill NaN values with the mean of each column\n",
    "df[\"col1\"].fillna(df[\"col1\"].mode()[0]) # Fill NaN values with the mode of the column\n",
    "                                        # Note that .mode() here returns a Series object so use [0] to select the (first) element/value \n",
    "# Dealing with dulplicates\n",
    "df.duplicated(subset=['col1', 'col2']) # Return boolean Series denoting duplicate rows\n",
    "df.drop_duplicates(subset=['col1,col2'],keep='last') \n",
    "# Change data type\n",
    "s.astype(float) # Convert the datatype of the Series to float\n",
    "\n",
    "# Replace values\n",
    "s.replace(1,\"one\") # Replace all 1 with \"one\"\n",
    "s.replace([1,3],[\"one\",\"three\"])\n",
    "df.replace(1,\"one\")\n",
    "df.replace([1,3],[\"one\",\"three\"])\n",
    "df.replace({1:\"one\", 3: \"three\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6b566-855d-49b0-9689-68f07269fc67",
   "metadata": {},
   "source": [
    "# Filter, sort, groupby and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce2e97d-101c-4a69-8d88-40b46da9cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "df[df['col']>0.5]  # Select the rows that meet the condition\n",
    "df[(df['col']>0.5) & (df['col']<1)]  # Select the rows that meet the condition\n",
    "df.loc[df['col']>0.5,:] \n",
    "df.where(df>0.5) # Keeps the original values where the condition is True and replaces values where the condition is False\n",
    "                 # By defualt it replaces with numpy.NaN \n",
    "df.where(df>0.5, other=0)\n",
    "\n",
    "# Sorting\n",
    "df.sort_index() # Sort by index along rows in ascending order\n",
    "df.sort_index(ascending=False, inplace=True) # Sort by index along rows in descending order, inplace change\n",
    "df.sort_values(by=\"col\",ascending=False) # Sort values by col in descending order; defacut ascending=True\n",
    "df.sort_values(by=[\"col1','col2'],ascending=[False, True] )\n",
    "df.reset_index() # Reset index\n",
    "df['col_rank']=df['col'].rank() # Creates a new column where each entry corresponds to the rank (1 through n) of that row’s value in column 'col'\n",
    "df['col_rank']=df['col'].rank(method='min',ascending=False) # the records that have the same values are ranked using the lowest rank; \n",
    "                                                            # By default the average rank is used; other methods are ‘max’, ‘first’, ‘dense’. \n",
    "# Grouping\n",
    "df.groupby('col') # Returns a groupby object for values from column with column label col; this did a mapping/splitting to the df\n",
    "df.groupby(['col1','col2']) \n",
    "df.groupby('col').size() # Return the number of rows in each group, a series\n",
    "df.groupby('col').count() # Retrun the number of NON_NULL values for each column\n",
    "df.groupby('col')['col2'].count() # Retrun the number of NON_NULL values for column 'col2'                  \n",
    "df.groupby('col').agg(np.mean) #  Produces a DataFrame with the group names as its new index and the mean values for each numeric column by group.\n",
    "df.groupby('col').mean() # mean(), median(), mode(), sum(), size(), count(), min(), max(), std(), \n",
    "                       # var() (computes the variance of each group), describe() (outputs descriptive statistics by group), \n",
    "                       # and nunique() (gives the number of unique values in each group)\n",
    "df.groupby('col1')['col2'].mean() #Return the mean of the values in col2, grouped by the values in col1, a series\n",
    "df.groupby('col1')[['col2', 'col3']].mean() #Return the mean of the values in col2 and col3, grouped by the values in col1, a dateframe\n",
    "df.groupby('col1')[['col2', 'col3']].agg([np.sum, np.mean, np.std]) # Apply several functions at once\n",
    "df.groupby('col1')[['col2', 'col3']].agg(['sum', 'mean', 'std']) # Apply functions at once, using pandas' optimized groupby sum(), mean(), std()methods\n",
    "df.groupby('col1').agg({\"col2\": \"mean\", \"col3\": \"std\"}) #  Apply \"mean\" to \"col2\", \"std\" to \"col3\"               \n",
    "df.groupby('col1')['col2'].transform(lambda x: (x - x.mean()) / x.std()) # Rreturn a new DataFrame with the same row numbers and indexing as the original one but with transformed individual values\n",
    "df.groupby('col1')['col2'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "df.groupby('col1').head(n) # Returns the first n rows (5, by default) of each group correspondingly\n",
    "\n",
    "# Encoding\n",
    "df = pd.get_dummies(df) # one-hot encoding to convert categorical variables into dummy/indicator variables\n",
    "\n",
    "# Other transformation\n",
    "df[['B', 'A']] = df[['A', 'B']]  # Swap column contents\n",
    "df.assign(name = [\"Emil\", \"Tobias\", \"Linus\"]) # Assign a new column \"name\" to a df, returning a new object with the new columns added to the original ones\n",
    "df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32) # Assign a new column \"temp_f\" from values of column \"temp_c\" via lambda function\n",
    "\n",
    "# use of .apply() on Series and DataFrames\n",
    "df.apply(my_function) # Apply my_function to each column\n",
    "df.apply(my_function, axis=1) # Apply my_function to each row\n",
    "df['FirstName'] = df['EmployeeName'].apply(lambda x : x.split()[0])#  Apply the lambda function on the 'EmployeeName' column to create a new column\n",
    "df['Value3']=df['Value1'].apply(lambda x: x**2)\n",
    "mask=df.apply(lambda x: True if x ['Gender'] == 'F' and x['Kids'] > 0 else False, axis=1) # return a boolean Series \n",
    "def calc_bmi(weight, height):  # creat a custom function \n",
    "    return np.round(weight/(height/100)**2, 2)\n",
    "df['BMI'] = df.apply(lambda x: calc_bmi(x['Weight'], x['Height']), axis=1) # Apply the custom function to the data frame, across each row\n",
    "\n",
    "# PIVOT_TABLE\n",
    "table=pd.pivot_table(df,index=['col1','col2'],values=['col3','col4'],columns='col5', aggfunc='sum') # Create a pivot table that groups by 'col1' and 'col2'; by default, aggfunc='mean'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7768fb-8a82-46de-a137-6a8c0268d6a3",
   "metadata": {},
   "source": [
    "# Join/Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c619fe-defd-47ae-95ff-d48d647c09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1,df2]) # Concatenate along rows, retain original index\n",
    "pd.concat([df1,df2],join=\"inner\",ignore_index=True) # Concatenate along rows and return only overlapping columns, reset index\n",
    "pd.concat([df1,df2],axis=1) # Concatenate along columns (only work if the tables have the same height)\n",
    "# Note df.append() had been deprecated\n",
    "\n",
    "pd.merge(left, right, on=\"key\") # Inner join on \"key\" in both dataframes\n",
    "pd.merge(left, right, how=\"left\", on=[\"key1\", \"key2\"]) # Left join on [\"key1\", \"key2\"] in left\n",
    "pd.merge(left, right, left_on=\"key\", right_index=True, how=\"left\", sort=False) # Join DataFrame left’s column \"key\" with DataFrame right’s index\n",
    "\n",
    "df1.merge(df2, left_on='lkey', right_on='rkey') # how=\"inner\" by default\n",
    "\n",
    "df1.join(df2, how=\"inner\") # .join() joins on indexes by default,  how=\"left\" by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326765ea-9295-40f4-9371-253f8d2367c7",
   "metadata": {},
   "source": [
    "# Statistics and some common operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf667c1-174a-4f95-96cc-9947b63ed006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum() # Defual axis=0, sum of columns\n",
    "df.min()\n",
    "df.max()\n",
    "df.median()\n",
    "df.mode() # Retruns the mode of each column, a DataFrame\n",
    "s.mode() # The mode is the value that appears most often. There can be multiple modes. Always returns Series even if only one value is returned.\n",
    "df.std()\n",
    "df.count() # Return the number of non-null values in each column\n",
    "q1 = df[column].quantile(0.25)\n",
    "q3 = df[column].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "df.nunique() # number of unique values\n",
    "df.corr() # Returns the correlation coefficients between columns\n",
    "df.copy()\n",
    "\n",
    "df.T # Transpose rows and cols\n",
    "df.size # nrows* ncols\n",
    "df.values # Get a numpy array for df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7963f6-cc3f-4a55-87fe-f0fed3a1d0d6",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b1b88-fb72-4c3d-8d78-c0f57256c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Boxplot\n",
    "#  to visualize distribution of `col1` and detect any outliers\n",
    "plt.figure(figsize=(5,1))\n",
    "plt.title('Boxplot to detect outliers', fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "sns.boxplot(x=df['col1'])\n",
    "plt.show()\n",
    "\n",
    "##  Histogram\n",
    "# Histogram for 'col1'\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.histplot(df['col1'], bins=range(start,stop,step)) # start inclusive, stop non-inclusive\n",
    "plt.title('col1 histogram');\n",
    "\n",
    "# Histogram for 'col1' and set the label format for x-axies\n",
    "ax = sns.histplot(df['col1'], bins=range(0,(7*10**5+1),10**5))\n",
    "labels = [0] + [str(i) + 'k' for i in range(100, 701, 100)]\n",
    "ax.set_xticks(range(0,7*10**5+1,10**5), labels=labels)\n",
    "plt.title('col1 histogram');\n",
    "\n",
    "# Histogram for 'col1' with hue mapping to 'col2'\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.histplot(data=df, x='col1', hue='col2', multiple='dodge', shrink=0.5)\n",
    "plt.title('col1 histogram')\n",
    "\n",
    "## Scatterplot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.scatterplot(data=df, x='col1', y='col3', hue='col2', s=10, alpha=0.4) # s for marker size, alpha for transparency degree\n",
    "plt.axvline(x=X, color='#ff6361', label='X', ls='--') # add a vertical dashline\n",
    "plt.title('col1 by col3', fontsize='14');\n",
    "\n",
    "## Pie graph\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "plt.pie(df.groupby('col1')['col2'].sum(), labels=['label1', 'label2'])\n",
    "plt.title('col2 by col1');\n",
    "\n",
    "## bar chart\n",
    "df2 = df.groupby(['col1']).median(\n",
    "    numeric_only=True).reset_index()\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.barplot(data=df2,\n",
    "            x='col1',\n",
    "            y='col2',\n",
    "            order=['active', 'under review', 'banned'],\n",
    "            palette={'active':'green', 'under review':'orange', 'banned':'red'},\n",
    "            alpha=0.5)\n",
    "plt.title('Median view count by col1');\n",
    "\n",
    "## Pairplot\n",
    "sns.pairplot(\n",
    "   df,\n",
    "   vars=[\"col1\", \"col2\", \"col3\", \"col4\", \"col5\"],\n",
    "   hue=\"col5\",\n",
    ");\n",
    "\n",
    "## Heatmap\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(df[['col1','col2','col3','col4']].corr(),annot=True, cmap=\"crest\")\n",
    "plt.title('Heatmap of the dataset')\n",
    "plt.show()\n",
    "\n",
    "## subplots\n",
    "# single Axes\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# multiple Axes\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "sns.histplot(df['col1'], bins=100,ax=ax[0])\n",
    "ax[0].set_title('title1', fontsize='14')\n",
    "sns.histplot(df['col2'], bins=100,ax=ax[1])\n",
    "ax[1].set_title('title2', fontsize='14')\n",
    "\n",
    "# using tuple unpacking for multiple Axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fa4f14-ab89-4fe4-88be-9183ef11609e",
   "metadata": {},
   "source": [
    "\n",
    "# DateTime in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2670b03-e3fe-41cb-a4ae-94fbbbc3b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pandas Timestamp object \n",
    "date = pd.Timestamp('2013-01-01')\n",
    "date2 = pd.Timestamp('2013-01-01 21:15:06') \n",
    "date3 = pd.Timestamp('Sep 04, 1982 1:35.18')\n",
    "# Create a Period object\n",
    "month = pd.Period('2013-01', freq='M') \n",
    "# Create a sequence of dates \n",
    "dates=pd.date_range('2022-2-7', periods=7) # Return a fixed frequency DatetimeIndex. Each date in the DatetimeIndex instance is an instance of the Timestamp.\n",
    "pd.date_range(start='1/1/2018', periods=5, freq='3ME') # 3 month end frequency, by default, freq='D'\n",
    "\n",
    "# Separate element of a Timestamp object from built-in attributes\n",
    "year=date.year\n",
    "month=date.month\n",
    "day=date.day\n",
    "hour=date.hour\n",
    "month_name=date.month_name()\n",
    "week_day=date.weekday() # Return the day of the week as a number,counting from 0 (for monday)\n",
    "day_name=date.day_name()\n",
    "\n",
    "# Convert to  a pandas datetime object\n",
    "df['datetime'] = pd.to_datetime(df['datetime'],yearfirst=True) # Convert to column 'datetime' (string object) to a datetime object\n",
    "mask = (df['datetime'] >= pd.Timestamp('2019-03-06')) & (df.datetime < pd.Timestamp('2019-03-07')) # Create a Boolean mask to select the DataFrame rows between two specific dates\n",
    "df[mask]\n",
    "df.set_index('datetime', inplace=True) # Set the datetime column as the index of the DataFrame for Timestamp slicing \n",
    "df.loc['03-04-2019':'04-04-2019'] # Return rows within a date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd8678-8a20-435e-b7b9-c2ecaaee3ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
